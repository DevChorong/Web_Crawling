# Beautiful soup 와 request 함수를 <br>사용한 네이버 영화 평점 크롤링

### # 사용한 툴

> * Pycharm - python
> * R studio - R x64



### INDEX

> 1. 어떻게?? 어떻게 할 생각이지?
> 2. 파이썬과 파이참 그리고 라이브러리 설치
> 3. request 라이브러리 란?
> 4. Beatiful soup4란?
> 5. WordCloud란?

<br>

### 1. 어떻게?? 어떻게 할 생각이지?

코드를 짜는 순서는 다음과 같이한다.

> 1. request 라이브러리 함수를 사용해서 내가 얻고자 하는 헤이지의 HTML 소스코드를 가져온다.
> 2. beautiful soup 로 크롤링 한 HTML소스코드를 내가 원하는 문장 DATA 만 뽑아낸다
> 3. 그것을 하나의 배열로 만든다.
> 4. 문장 데이터를 연결시켜서 하나의 문장 변수에 저장한다.
> 5. 저장한 문장 변수를 txt 파일로 만든다.
> 6. txt파일을 R studio에 불러서 Wordcloud를 만든다.

<br>

### 2. 파이썬과 파이참 그리고 라이브러리 설치

******

우리가 사용할 라이브러리는 총 3가지 이다

> 1. HTML소스코드 불러오기용 라이브러리
> 2. 불러온 소스코드중 필요한 데이터만 추출하기 위한 라이브러리

1번에 해당하는 것이 바로 <mark>request 라이브러리</mark>이다.

2번에 해당하는 것이 바로 <mark>Beautiful Soup 4 라이브러리</mark>이다.

<br>

#### # 라이브러리 설치과정

> 1. 먼저 파이썬과 파이참을 설치한다. (파이썬 -> 파이참)
> 2. 파이참을 실행한뒤 
> 3. File -> Setting(Ctrl + Alt + s) -> Project : 프로젝트명(version control 밑에) -> Project Interpreter
> 4. 오른쪽 화면을 보면 + 모양이 있다. 그것을 누른다.
> 5. 위에 검색창에 다음 2가지를 입력한 뒤에 왼쪽 아래에 Install Package 를 누른다.
>    1. request
>    2. Beautiful Soup 4
> 6. 설치가 완료되면 그걸로 라이브러리 설치는 마무리 한다.

<br>

### 3. request 라이브러리 [ request = 요구 / 요청 ]

******

request 라이브러리란 한마디로 정의하자면 다음 과 같다.

#### "  Python에서 HTTP 요청을 보내는 모듈 "

[ TMI ] HTTP는 (**H**yper**T**ext **T**ransfer **P**rotocol)  WWW 상에서 정보를 주고받을 수 있는 프로토콜이다. 

다음의 소스코드를 보자

<br>

```Python
import requests
URL = 'http://www.naver.com'

connect = requests.get(URL)

print(connect.status_code)

html = connect.text
print(html)
```

##### 1.<mark> request.get(URL)</mark>

 * 해당 URL의 서버와의 HTTP요청 허가를 가져온다
 * 출력하면 <Response [200]>을 출력한다.
 * 위의 소스코드를 보면 connect라는 변수에다가 요청허가를 저장해 두었다.
 * "connect.~~~" 를 해석할때는 "~(을)를 HTTP에 요청합니다" 로 해석하면 이해하기 쉽다.

<br>

##### 2. connect<mark>.status_code</mark>

* 받아온 URL의 HTTP요청을 정상적으로 서버와 주고받는지(통신하는지) 확인하는 일종의 확인 함수이다.
* 정상적으로 작동한다면 200을 리턴한다!
* "HTTP에 요청하기를 상태코드"

<br>

##### 3. connect<mark>.txt</mark>

* 받아온 URL의 HTML소스코드를 HTTP에게 요청하는 함수이다.
* 정상적으로 작동한다면 크롬에서 "소스보기"를 했을 때와 같은 결과 값이 나온다.
* 위 소스코드를 보면 html이라는 변수에다가  받아온 URL의 HTML소스코드 를 txt형태로 저장했다.
* 이때 type(html)을 해보면 string 타입으로 저장 됨을 알 수 있다.
* "HTML로된 소스코드 txt파일을 HTTP에 요청합니다"

<br>

그렇다면 request.get()에 사용할 URL을 선택해보자!!

1. 먼저 <mark>네이버 영화</mark>페이지 에들어가서 자신이 원하는 영화를 검색한다.
2. 검색한 영화페이지 에서 평점을 클릭한다
3. 쭉 내려서 1~10번까지 넘길 수 있는 배너를 오른쪽 클릭하고 검사(Ctrl + shift + i)를 누른후에
4. 나오는 여러가지 페이지 중에서 하나를 클릭한다.
5. 그다음 첫번째 페이지로 이동한다.

<br>

네이버 영화 평론 페이지는 다음과 같은 URL 구성으로 되어있다!

```URL
https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=163608&type=after&onlyActualPointYn=Y&page=1
```

이때 우리가 주목 해야할 것은 다음과 같다.

```URL
?code=163608

&page=1
```

code 부분을 보면 영화는 영화마다 code를 갖는다 따라서 내가 선택한 영화(돈, 아이언맨3)는 다음의 코드르 가져졌다

> * 아이언맨3 = 70254
> * 돈 = 163608

**그리고 우리가 무조건 봐야하는!!! 가장 중요한 것은 바로 저 <mark>&page=1</mark>부분 이다.**

**저부분은 평점 페이지가다음으로 넘어갈 때마다 1씩 증가한다. 따라서 우리가 페이지 1의 평론부터**

**가장 마지막 페이지의 평론까지 모두 크롤링 하려면 이부분을 for문을 통해서 1부터 끝까지 반복해주면 된다.**

<br>

### 4. Beatiful soup4란?

******

request 라이브러리 를 통해서 크롤링 해온 소스코드중 **내가 원하는 데이터만 뽑아내는 것(Parsing)**을 도와주는  소스코드 파싱 라이브러리 이다.

다음의 소스코드를 보자

```python
soup = BeautifulSoup(htme_code, 'html.parser')

review = soup.select('CSS Selector')
```

##### <mark>1. 객체용 변수 = BeautifulSoup(소스코드가 저장된 변수 , 'html.parser')</mark>

위 작업은 "소스코드가 저장된 변수" 로부터 소스코드를 받아서 BeautifulSoup의 함수들을 사용하여 편집할 수 있는 하나의 객체로 전환 시켜주는 작업이다.

또한 html.parser이란 python 내부에 있는 html을 건드릴 수 있는 함수로 이부분을 뒤로 내가 선언한 변수를 통해서 html을 터치할 수 있다.



<br>

### 5. WordCloud란?

******

##### 1. R studio 패키지 설치 및 사용준비

```R
# 가장 중요한 한글 체킹 패키지
install.packages("KoNLP")
install.packages("rJava")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_201') # jre의 위치 
library("KoNLP")
useSejongDic()

# 나머지 패키지들 import

install.packages("SnowballC")	#  SnowballC는 이 단어들을 뭉쳐줄 수 있는 패키지
install.packages("wordcloud")	#  WordCloud를 만들어주는 패키지
install.packages("RColorBrewer")#  wordCloud의 색깔을 넣어주는 패키지 # display.brewer.all()
install.packages("plyr")
install.packages("stringr")
install.packages("ggplot2")

# import 한 패키지들 사용
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("plyr")
library("ggplot2")

# Register nouns
mergeUserDic(data.frame("사전이름"", "ncn"))

```

<br>

##### 2. 파일 적재 

```R
# 자료파일의 위치를 지정(Work Directory)
setwd("C:\\Users\\ChoRong\\Desktop\\Base\\04. 개인 연구\\[4]Web_Crawling")

# Textfile 가져오기
text <- readLines("review.txt")
```

<br>

##### 3. 문장을 단어로 나누기 및 필요없는 단어 삭제

```R
# 명사 단위로 쪼개기
text <- sapply(text, extractNoun, USE.NAMES = F)

# 리스트 해제 하고 2개의 길이만 허용하는 필터 = [1차원 형태로 만듦]
text <- unlist(text) 
text <- Filter(function(x) {nchar(x) >= 2}, text) # 2글자 이상만 취급

# 필요없는 단어 삭제하기
text <- str_replace_all(text,"[^[:alpha:]]","")
text <- str_replace_all(text,"[A-Za-z0-9]","")
text <- gsub("을", "", text)
text <- gsub("를", "", text)
text <- gsub("ㅋ", "", text)
text <- gsub("ㅠ", "", text)
text <- gsub("ㅇ", "", text)
text <- gsub("영화", "", text)

```

<br>

##### 4. 배열을 [word] 와 [frq]를 요소로 갖는 DataFrame 형태로 만들기

```R

# 임시적인 파일을 만들어서 테이블 형태로 불러오기.
write(unlist(text) , "kr_text.txt")
text_table <- read.table("kr_text.txt")

# 테이블 형태로 데이터 만들기
word_count <- table(text_table)

# dataFrame 만들기
terms <- data.frame(word_count)

# 열 이름 바꿔주기
names(terms) <- c("word", "freq")

# 정렬 해주기
terms <- arrange(terms, desc(freq)) # 내림 차순

terms # 출력해서 어떤 형태인지 확인하기
```

##### 5. wordcloud 출력

```R
wordcloud(words=terms$word, freq=terms$freq, random.order = FALSE, colors=brewer.pal(8, "Dark2"))
```



















