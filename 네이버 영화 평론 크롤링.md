# Beautiful soup 와 request 함수를 <br>사용한 네이버 영화 평점 크롤링

### # 사용한 툴

> * Pycharm - python
> * R studio - R x64

### # 사용한 패키지 / 라이브러리

> * Python
>   * request : html 소스코드를 받아오는 일
>   * Beautiful Soup 4 : 받아온 소스코드중에서 필요한 데이터만 추출
> * R language
>   * wordcloud : Word Cloud를 시각적으로 만들어 내는 패키지
>   * ggplot2 : 기본 R 그래픽스에서 제공하는 대부분의 작업을 효과적으로 수행한다.
>   * stringr : 문자열 함수들을 사용할 수 있음
>   * SnowballC : 단어들을 뭉쳐줄 수 있는 패키지
>   * RColorBrewer : 색상 탬플릿을사용할 수 있는 패키지
>   * plyr : 원본 데이터를 분석하기 쉬운 형태로 나누어서 다시 새로운 형태로 만들어 주는 유용한 패키지

<br>

## INDEX

> 1. 어떻게?? 어떻게 할 생각이지?
>
> 
>
> 1. Pycharm으로 데이터 모으기
>
>    1. 파이썬과 파이참 그리고 라이브러리 설치
>    2. request 라이브러리 란?
>    3. Beatiful soup4란?
>    4. 문장만 따는 함수 만들기
>
> 
>
> 2. R Studio 로 시각화 하기
>
>    1. Word Cloud 란?
>    2. KoNLP 란?
>    3. 직접 해보기

<br>

### 1. 어떻게?? 어떻게 할 생각이지?

******

코드를 짜는 순서는 다음과 같이한다.

> 1. Pycharm
>
>    1. request 라이브러리 함수를 사용해서 내가 얻고자 하는 헤이지의 HTML 소스코드를 가져온다.
>    2. beautiful soup 로 크롤링 한 HTML소스코드를 내가 원하는 문장 DATA 만 뽑아낸다
>    3. 그것을 하나의 배열로 만든다.
>    4. 문장 데이터를 연결시켜서 하나의 문장 변수에 저장한다.
>    5. 저장한 문장 변수를 txt 파일로 만든다.
>
>    
>
> 2. R Studio
>
>    1. txt파일을 R studio에 불러서 Wordcloud를 만든다.

<br>

## 2. Pycharm으로 데이터 모으기

### 2. 1. 파이썬과 파이참 그리고 라이브러리 설치

******

우리가 사용할 라이브러리는 총 2가지 이다

> 1. HTML소스코드 불러오기용 라이브러리
> 2. 불러온 소스코드중 필요한 데이터만 추출하기 위한 라이브러리

1번에 해당하는 것이 바로 <mark>request 라이브러리</mark>이다.

2번에 해당하는 것이 바로 <mark>Beautiful Soup 4 라이브러리</mark>이다.

<br>

#### # 라이브러리 설치과정

> 1. 먼저 파이썬과 파이참을 설치한다. (파이썬 -> 파이참)
> 2. 파이참을 실행한뒤 
> 3. File -> Setting(Ctrl + Alt + s) -> Project : 프로젝트명(version control 밑에) -> Project Interpreter
> 4. 오른쪽 화면을 보면 + 모양이 있다. 그것을 누른다.
> 5. 위에 검색창에 다음 2가지를 입력한 뒤에 왼쪽 아래에 Install Package 를 누른다.
>    1. request
>    2. Beautiful Soup 4
> 6. 설치가 완료되면 그걸로 라이브러리 설치는 마무리 한다.

<br>

### 2. 2. request 라이브러리 [ request = 요구 / 요청 ]

******

request 라이브러리란 한마디로 정의하자면 다음 과 같다.

#### "  Python에서 HTTP 요청을 보내는 모듈 "

[ TMI ] HTTP는 (**H**yper**T**ext **T**ransfer **P**rotocol)  WWW 상에서 정보를 주고받을 수 있는 프로토콜이다. 

다음의 소스코드를 보자

<br>

```Python
import requests
URL = 'http://www.naver.com'

connect = requests.get(URL)
print(connect)

html = connect.text
print(html)
```

##### 1.<mark> request.get(URL)</mark>

 * 해당 URL의 서버와의 HTTP요청 허가를 가져온다
 * 출력하면 <Response [200]>을 출력한다.
 * 위의 소스코드를 보면 connect라는 변수에다가 요청허가를 저장해 두었다.
 * "connect.~~~" 를 해석할때는 "~(을)를 HTTP에 요청합니다" 로 해석하면 이해하기 쉽다.

<br>

##### 2. connect<mark>.txt</mark>

* 받아온 URL의 HTML소스코드를 HTTP에게 요청하는 함수이다.
* 정상적으로 작동한다면 크롬에서 "소스보기"를 했을 때와 같은 결과 값이 나온다.
* 위 소스코드를 보면 html이라는 변수에다가  받아온 URL의 HTML소스코드 를 txt형태로 저장했다.
* 이때 type(html)을 해보면 string 타입으로 저장 됨을 알 수 있다.
* "HTML로된 소스코드 txt파일을 HTTP에 요청합니다"

<br>

그렇다면 request.get()에 사용할 URL을 선택해보자!!

1. 먼저 <mark>네이버 영화</mark>페이지 에들어가서 자신이 원하는 영화를 검색한다.
2. 검색한 영화페이지 에서 평점을 클릭한다
3. 쭉 내려서 1~10번까지 넘길 수 있는 배너를 오른쪽 클릭하고 검사(Ctrl + shift + i)를 누른후에
4. 나오는 여러가지 페이지 중에서 하나를 클릭한다.
5. 그다음 첫번째 페이지로 이동한다.

<br>

네이버 영화 평론 페이지는 다음과 같은 URL 구성으로 되어있다!

```URL
https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=163608&type=after&onlyActualPointYn=Y&page=1
```

이때 우리가 주목 해야할 것은 다음과 같다.

```URL
?code=163608

&page=1
```

code 부분을 보면 영화는 영화마다 code를 갖는다 따라서 내가 선택한 영화(돈, 아이언맨3)는 다음의 코드르 가져졌다

> * 아이언맨3 = 70254
> * 돈 = 163608

**그리고 우리가 무조건 봐야하는!!! 가장 중요한 것은 바로 저 <mark>&page=1</mark>부분 이다.**

**저부분은 평점 페이지가다음으로 넘어갈 때마다 1씩 증가한다. 따라서 우리가 페이지 1의 평론부터**

**가장 마지막 페이지의 평론까지 모두 크롤링 하려면 이부분을 for문을 통해서 1부터 끝까지 반복해주면 된다.**

<br>

### 2. 3. Beatiful soup4란?

******

request 라이브러리 를 통해서 크롤링 해온 소스코드중 **내가 원하는 데이터만 뽑아내는 것(Parsing)**을 도와주는  소스코드 파싱 라이브러리 이다.

다음의 소스코드를 보자

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(htme_code, 'html.parser')

review = soup.select('CSS Selector')
```

##### <mark>1. 객체용 변수 = BeautifulSoup(소스코드가 저장된 변수 , 'html.parser')</mark>

위 작업은 "소스코드가 저장된 변수" 로부터 소스코드를 받아서 BeautifulSoup의 함수들을 사용하여 편집할 수 있는 하나의 객체로 전환 시켜주는 작업이다.

또한 html.parser이란 python 내부에 있는 html을 건드릴 수 있는 함수로 이부분을 뒤로 내가 선언한 변수를 통해서 html을 터치할 수 있다.

<br>

##### <mark>2. 선택된 것들을 모을 변수 = 객체용 변수.select(' CSS selector ')</mark> 

위 작업은 앞서 선언해준 객체용 변수"소스코드가 저장된 변수"에서!! select라는 기능을 사용할 수 있도록 허가를 후는 것이다!

select기능 : 뒤에 있는 CSS selector을 갖는 html 소스코드를 추출해준다. 

#### # CSS selector 뽑아내는 방법

> 1. 크롬을 켠다
> 2. 원하는 페이지를 오른쪽 클릭해서 검사(Ctrl + shift + i)를 누른다
> 3. 원하는 부분을 오른쪽 클릭해서 Copy -> Copy Selector
> 4. 네이버 로고의 경우 다음의 CSS selector을 갖는다.
>    1. #PM_ID_ct > div.header > div.special_bg > div > div.area_logo > h1 > a > span
> 5. 이때 . 뒤에 나온 모든것들은 지우고 기본적인 태그형태만 남긴다.
>    1. #PM_ID_ct > div> div > div > div > h1 > a > span

<br>

## 3. R Studio 로 시각화 하기

### 3. 1. WordCloud란?

******

WordCloud 란 문서의 키워드, 개념 등을 직관적으로 파악할 수 있도록 핵심 단어를 시각적으로 돋보이게 하는 기법이다. 

### 3. 2. KoNLP란?

******

한글 자연어 분석 패키지인 **KoNLP**(Korean Natural Language Processing)패키지에는 한국어를 분석할 수 있는 총 27개의 패키지가 들어있다 - GOOGLE

### 3. 3. 직접 해보기

##### 1. R studio 패키지 설치 및 사용준비

```R
# wordcloud를 위한 패키지들 import
install.packages("stringr")
install.packages("ggplot2")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("RColorBrewer")
install.packages("plyr")


# 가장 중요한 한글 체킹 패키지
install.packages("KoNLP")
install.packages("rJava")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_201')
library("KoNLP") # 바로 패키지 실행
useSejongDic()

# 사용자이름 에 해당하는 data.frame의 사전을 하나 생성한다.
mergeUserDic(data.frame("사전이름", "ncn"))

# import 한 패키지들 사용
library("stringr")
library("ggplot2")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("plyr")
```

<br>

##### 2. Working Directory속 파일 적재(Load) 

```R
# 자료파일의 위치를 지정(Work Directory)
setwd("C:\\Users\\ChoRong\\Desktop\\Base\\04. 개인 연구\\[4]Web_Crawling")

# Textfile 가져오기
text <- readLines("review.txt")
```

<br>

##### 3. 문장을 단어로 나누기 및 필요없는 단어 삭제

```R
# 명사 단위로 쪼개기
text <- sapply(text, extractNoun, USE.NAMES = F)

# 리스트 해제 하고 2개의 길이만 허용하는 필터 = [1차원 형태로 만듦]
text <- unlist(text) 
text <- Filter(function(x) {nchar(x) >= 2}, text) # 2글자 이상만 취급

# 필요없는 단어 삭제하기
text <- str_replace_all(text,"[^[:alpha:]]","") 	# 특수문자 삭제
text <- str_replace_all(text,"[A-Za-z0-9]","")		# 숫자로 시작하는 단어 삭제
text <- gsub("을", "", text)
text <- gsub("를", "", text)
text <- gsub("ㅋ", "", text)
text <- gsub("ㅠ", "", text)
text <- gsub("ㅇ", "", text)
text <- gsub("영화", "", text)

```

<br>

##### 4. 배열을 [word] 와 [frq]를 요소로 갖는 DataFrame 형태로 만들기

```R

# 임시적인 파일을 만들어서 테이블 형태로 불러오기.
write(unlist(text) , "kr_text.txt")
text_table <- read.table("kr_text.txt")

# 테이블 형태로 데이터 만들기
word_count <- table(text_table)

# dataFrame 만들기
terms <- data.frame(word_count)

# 열 이름 바꿔주기
names(terms) <- c("word", "freq")

# 정렬 해주기
terms <- arrange(terms, desc(freq)) # 내림 차순

terms # 출력해서 어떤 형태인지 확인하기
```

##### 5. wordcloud 출력

```R
wordcloud(words=terms$word, freq=terms$freq, random.order = FALSE, colors=brewer.pal(8, "Dark2"))
```







******

# 전체 소스코드

### Python

```python
import requests
from bs4 import BeautifulSoup


## 관람객 평점만 보기

def get_text(text):
    s = str(text)                  # 입력받은 것의 타입이 무엇이든 간에 String 타입으롭 바꿔준다.
    start = s.find("관람객</span>") + len("관람객</span>")
    end = s.find("</p>")
    result = s[start:end]
    return result


data = [] # 크롤링 한 140자 평론 데이터를 모으는 배열

for i in range(1,233):   # 개발자가 직접 확인해본 페이지
    URL = 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=163608&type=after&onlyActualPointYn=Y&page=' + str(i)
    # request 라이브러리 파트
    connect = requests.get(URL)
    html = connect.text
	
	# BS4 패키지 파트
    soup = BeautifulSoup(html, 'html.parser')
    sentence = soup.select(
        'body > div > div > div > ul > li > div > p'
        # 하게되면 soup 라는 패키지가 알아서 my_titles 에 h3 > a 라는 selector을 가진 것을 모아서 my_titles 에 넣는다.
    )
    
    # soup를 통해서 가져온 문장에서 html 태그를 삭제하고 온전한 문장만을 꺼내오는 작업
    for j in range(len(sentence)):
        sentence[j] = get_text(sentence[j])
        data.append(sentence[j])

# 추출한 모든 문장을 1개의 문장으로 연결 시켜주는 작업
text = data[0]

for k in range(1,len(data)):
    text = text + " "+ data[k]

print(text)


```

<br>

### R

```R

# wordcloud를 위한 패키지들 import
install.packages("stringr")
install.packages("ggplot2")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("RColorBrewer")
install.packages("plyr")


# 가장 중요한 한글 체킹 패키지
install.packages("KoNLP")
install.packages("rJava")
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_201')
library("KoNLP")
useSejongDic()

# 사용자이름 에 해당하는 data.frame의 사전을 하나 생성한다.
mergeUserDic(data.frame("사전이름", "ncn"))

# import 한 패키지들 사용
library("stringr")
library("ggplot2")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("plyr")


setwd("C:\\Users\\ChoRong\\Desktop\\Base\\04. 개인 연구\\[4]Web_Crawling")
# Textfile 가져오기
text <- readLines("review.txt")

# 명사 단위로 쪼개기
text <- sapply(text, extractNoun, USE.NAMES = F)

# 리스트 해제 하고 2개의 길이만 허용하는 필터 = [1차원 형태로 만듦]
text <- unlist(text)
text <- Filter(function(x) {nchar(x) >= 2}, text) 

# 필요없는 단어 삭제하기
text <- str_replace_all(text,"[^[:alpha:]]","") # 특수문자
text <- str_replace_all(text,"[A-Za-z0-9]","")  # 숫자
text <- gsub("을", "", text)
text <- gsub("를", "", text)
text <- gsub("ㅋ", "", text)
text <- gsub("ㅠ", "", text)
text <- gsub("ㅇ", "", text)
text <- gsub("영화", "", text)
# 임시적인 파일을 만들어서 테이블 형태로 불러오기.
write(unlist(text) , "kr_text.txt")
text_table <- read.table("kr_text.txt")
# 테이블 형태로 데이터 만들기
word_count <- table(text_table)

# dataFrame 만들기
terms <- data.frame(word_count)

# 열 이름 바꿔주기
names(terms) <- c("word", "freq")

# 정렬 해주기
terms <- arrange(terms, desc(freq))

terms

wordcloud(words=terms$word, freq=terms$freq, random.order = F, colors=brewer.pal(12,"Paired"))

```

















